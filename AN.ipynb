{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from iqtools import plotters, tools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "%matplotlib inline\n",
    "import iqtools as iq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = \"/Users/carloforconi/Desktop/PhD/coding/results/Mo/files/98Mo_processed_spectrograms_5kHz_crop.npz\"\n",
    "file = \"/Users/carloforconi/Desktop/PhD/coding/scripts/AN/files/98Mo_9.npz\"\n",
    "loaded_data = np.load(file, allow_pickle=True)\n",
    "\n",
    "f_axis = loaded_data[\"f_axis\"]\n",
    "t_axis = loaded_data[\"t_axis\"]\n",
    "summed_specs = loaded_data[\"summed_specs\"]\n",
    "filenames = loaded_data[\"filenames\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtract the baseline from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import sys\n",
    "sys.path.append(\"/Users/carloforconi/Desktop/PhD/coding/git/baseline-estimate\")\n",
    "from nonparams_est import NONPARAMS_EST\n",
    "\n",
    "for i in range(len(summed_specs)):\n",
    "    for j in range(len(summed_specs[0,i])):\n",
    "        baseline_estimator = NONPARAMS_EST(summed_specs[i][:, j])\n",
    "        b_data = baseline_estimator.pls(method='arPLS', l=1e4, ratio=1e-6)\n",
    "        summed_specs[i][:, j] = summed_specs[i][:, j] - b_data\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of one random spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum = np.sum(summed_specs[-1], axis=0)\n",
    "idx_gs = np.argmax(spectrum)\n",
    "df = f_axis[0][1] - f_axis[0][0]\n",
    "offset_hz = 1800   # adjust if needed\n",
    "offset_bins = int(offset_hz / abs(df))\n",
    "idx_iso_center = idx_gs - offset_bins\n",
    "half_width = 5\n",
    "idx_1 = idx_iso_center - half_width\n",
    "idx_2 = idx_iso_center + half_width\n",
    "slice_i = slice(idx_1, idx_2)\n",
    "bkg_width = half_width*2\n",
    "slice_bkg = slice(idx_1 - bkg_width, idx_1)\n",
    "slice_show = slice(idx_iso_center - 30, idx_iso_center + 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax = plt.subplots(figsize=(8, 6))  # adjust width and height as needed\n",
    "\n",
    "plotters.plot_spectrogram(f_axis[:,slice_show], t_axis[:,slice_show], summed_specs[-1][:,slice_show], \n",
    "                cen=0.0,      \n",
    "                cmap=\"jet\",     # colour scheme\n",
    "                dpi=300,        # png resolution\n",
    "                dbm=False,      # display in dBm scale\n",
    "                filename=None,  # if None, no file is produced\n",
    "                zzmin=1000,\n",
    "                zzmax=20000,\n",
    "                mask=False,     # mask out entries below this value\n",
    "                decimal_place=2\n",
    "                #title=f\"{name} \\n lframes={lframes}, nframes={nframes} \\n freq corr sum\"\n",
    "                )\n",
    "                \n",
    "#ax.set_xticks(ax.get_xticks()[::1])  # Show every 1 tick\n",
    "plt.xticks(rotation=45)  # Rotate labels for better readability\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x/1e0:.1f} Hz\"))\n",
    "plt.vlines(x = f_axis[0][idx_1], ymin=0, ymax=t_axis[-1][0], color='red')\n",
    "plt.vlines(x = f_axis[0][idx_2], ymin=0, ymax=t_axis[-1][0], color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decays detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def log_reject(t, step, noise, local_scale, m1, m2, reason):\n",
    "    print(f\"[REJECT] t={t:.6f}  \"\n",
    "          f\"step={step:.3e}  \"\n",
    "          f\"noise={noise:.3e}  \"\n",
    "          f\"local_scale={local_scale:.3e}  \"\n",
    "          f\"max={max(abs(m1), abs(m2)):.3e}  \"\n",
    "          f\"reason={reason}\")\n",
    "\n",
    "def log_accept(t, step, noise, local_scale, m1, m2):\n",
    "    print(f\"[ACCEPT] t={t:.6f}  \"\n",
    "          f\"step={step:.3e}  \"\n",
    "          f\"noise={noise:.3e}  \"\n",
    "          f\"local_scale={local_scale:.3e}  \"\n",
    "          f\"max={max(abs(m1), abs(m2)):.3e}\")\n",
    "\n",
    "# main function that detects the ion decays and also rejects false decays.\n",
    "def detect_all_ion_decays(time, f_axis, y_sum, y, slice_i, slice_bkg, \n",
    "                          min_points=5,\n",
    "                          min_step_factor=1.0,\n",
    "                          use_smoothing=True,\n",
    "                          smooth_window=7,\n",
    "                          require_decrease=False,\n",
    "                          min_rel_step=None,\n",
    "                          min_drop=0.2,\n",
    "                          local_factor=0.49,\n",
    "                          max_factor=0.29\n",
    "                          ):\n",
    "\n",
    "    time = np.asarray(time)\n",
    "    y_sum = np.asarray(y_sum)\n",
    "    n = len(y_sum)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Compute areas (signal + background)\n",
    "    # ---------------------------------------------------------\n",
    "    A_i_list = []\n",
    "    A_bkg_list = []\n",
    "\n",
    "    freq_slice = f_axis[slice_i]\n",
    "    freq_bkg   = f_axis[slice_bkg]\n",
    "\n",
    "    order     = np.argsort(freq_slice)\n",
    "    order_bkg = np.argsort(freq_bkg)\n",
    "\n",
    "    for i in range(n):\n",
    "        y_slice = y[slice_i, i][order]\n",
    "        A_i = np.trapezoid(y_slice, freq_slice[order])\n",
    "        A_i_list.append(A_i)\n",
    "\n",
    "        y_slice_bkg = y[slice_bkg, i][order_bkg]\n",
    "        A_bkg = np.trapezoid(y_slice_bkg, freq_bkg[order_bkg])\n",
    "        A_bkg_list.append(A_bkg)\n",
    "\n",
    "    A_i_list   = np.array(A_i_list)\n",
    "    A_bkg_list = np.array(A_bkg_list)\n",
    "\n",
    "    base_trace = A_i_list - A_bkg_list\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Smoothing for segmentation\n",
    "    # ---------------------------------------------------------\n",
    "    if use_smoothing:\n",
    "        w = max(3, int(smooth_window))\n",
    "        if w % 2 == 0:\n",
    "            w += 1\n",
    "        y_seg = savgol_filter(base_trace, window_length=w, polyorder=2)\n",
    "    else:\n",
    "        y_seg = base_trace\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. REWRITTEN SEGMENTATION (THIS IS THE FIX)\n",
    "    # ---------------------------------------------------------\n",
    "    def split_segment(start, end):\n",
    "        length = end - start\n",
    "        if length < 2 * min_points:\n",
    "            return [start, end]\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 3.1 Find best split index by SSE minimization\n",
    "        # ---------------------------------------------------------\n",
    "        best_idx = None\n",
    "        best_sse = np.inf\n",
    "\n",
    "        for k in range(start + min_points, end - min_points):\n",
    "            left  = y_seg[start:k]\n",
    "            right = y_seg[k:end]\n",
    "\n",
    "            m1 = left.mean()\n",
    "            m2 = right.mean()\n",
    "\n",
    "            sse = ((left - m1)**2).sum() + ((right - m2)**2).sum()\n",
    "\n",
    "            if sse < best_sse:\n",
    "                best_sse = sse\n",
    "                best_idx = k\n",
    "\n",
    "        if best_idx is None:\n",
    "            return [start, end]\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 3.2 Evaluate the step on UNSMOOTHED base_trace\n",
    "        # ---------------------------------------------------------\n",
    "        left  = base_trace[start:best_idx]\n",
    "        right = base_trace[best_idx:end]\n",
    "\n",
    "        m1 = left.mean()\n",
    "        m2 = right.mean()\n",
    "        step = abs(m1 - m2)\n",
    "\n",
    "        # Robust noise estimate\n",
    "        def robust_std(x):\n",
    "            med = np.median(x)\n",
    "            mad = np.median(np.abs(x - med))\n",
    "            return 1.4826 * mad\n",
    "\n",
    "        noise = max(robust_std(left), robust_std(right))\n",
    "        if noise == 0:\n",
    "            return [start, end]\n",
    "\n",
    "        # --- HYBRID LOCAL SCALE FIX ---\n",
    "        segment_trace = base_trace[start:end]\n",
    "\n",
    "        # local estimate\n",
    "        if len(segment_trace) > 2:\n",
    "            local_seg_scale = np.median(np.abs(np.diff(segment_trace)))\n",
    "        else:\n",
    "            local_seg_scale = 0\n",
    "\n",
    "        # global estimate\n",
    "        global_scale = np.median(np.abs(np.diff(base_trace)))\n",
    "\n",
    "        # hybrid scale: local, but never too small\n",
    "        local_scale = max(local_seg_scale, 0.60 * global_scale)\n",
    "        # --- END FIX ---\n",
    "\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 3.3 REJECTION CONDITIONS — STOP RECURSION COMPLETELY\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        t_split = time[best_idx]\n",
    "\n",
    "        if require_decrease and not (m2 < m1):\n",
    "            log_reject(t_split, step, noise, local_scale, m1, m2, \"no decrease\")\n",
    "            return [start, end]\n",
    "\n",
    "        if step < local_factor * local_scale:\n",
    "            print(\"step: \", step)\n",
    "            print(\"cost*local scale: \",local_factor * local_scale)\n",
    "            log_reject(t_split, step, noise, local_scale, m1 , m2, f\"step < {local_factor} * local_scale\")\n",
    "            return [start, end]\n",
    "\n",
    "        if step < max_factor * max(abs(m1), abs(m2)):\n",
    "            print(\"step: \", step)\n",
    "            print(\"cost*max(abs(m1), abs(m2)): \", max_factor * max(abs(m1), abs(m2)))\n",
    "            log_reject(t_split, step, noise, local_scale, m1, m2, f\"step < {max_factor} * max plateau\")\n",
    "            return [start, end]\n",
    "\n",
    "        if step < min_step_factor * noise:\n",
    "            print(\"step: \", step)\n",
    "            print(\"min_step_factor * noise: \", min_step_factor * noise)\n",
    "            log_reject(t_split, step, noise, local_scale, m1, m2, \"step < noise threshold\")\n",
    "            return [start, end]\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 3.4 ACCEPTED SPLIT → recurse\n",
    "        # ---------------------------------------------------------\n",
    "        log_accept(t_split, step, noise, local_scale, m1, m2)\n",
    "        left_bounds  = split_segment(start, best_idx)\n",
    "        right_bounds = split_segment(best_idx, end)\n",
    "\n",
    "        return left_bounds[:-1] + right_bounds\n",
    "\n",
    "    # Run segmentation\n",
    "    boundaries = sorted(set(split_segment(0, n)))\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Compute stats\n",
    "    # ---------------------------------------------------------\n",
    "    stats = []\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        s = boundaries[i]\n",
    "        e = boundaries[i + 1]\n",
    "\n",
    "        stats.append({\n",
    "            \"t_start\": time[s],\n",
    "            \"t_end\":   time[e - 1],\n",
    "            \"mean_y\":  y_sum[s:e].mean(),\n",
    "            \"std_y\":   y_sum[s:e].std(ddof=1),\n",
    "            \"mean_area\":      A_i_list[s:e].mean(),\n",
    "            \"mean_area_bkg\":  A_bkg_list[s:e].mean()\n",
    "        })\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 5. GLOBAL AREA-DROP FILTER (remove fake decays)\n",
    "    # ---------------------------------------------------------\n",
    "    if len(stats) > 1:\n",
    "        area_drops = [abs(stats[i+1][\"mean_area\"] - stats[i][\"mean_area\"])\n",
    "                    for i in range(len(stats)-1)]\n",
    "\n",
    "        median_drop = np.median(area_drops)\n",
    "        MIN_DROP = min_drop * median_drop\n",
    "        print(\"min drop: \", MIN_DROP)\n",
    "\n",
    "        new_stats = [stats[0]]\n",
    "        new_boundaries = [boundaries[0]]   # start boundary only\n",
    "\n",
    "        for i in range(1, len(stats)):\n",
    "            drop = abs(stats[i][\"mean_area\"] - stats[i-1][\"mean_area\"])\n",
    "\n",
    "            # actual decay time between plateau i-1 and i\n",
    "            decay_idx = boundaries[i]\n",
    "            decay_time = time[decay_idx]\n",
    "\n",
    "            print(f\"t = {decay_time:.6f} drop: {drop}\")\n",
    "\n",
    "            if drop >= MIN_DROP:\n",
    "                # keep plateau i\n",
    "                new_stats.append(stats[i])\n",
    "                new_boundaries.append(boundaries[i])   # <-- FIXED\n",
    "            else:\n",
    "                print(f\"level dropped at decay time ≈ {decay_time:.6f} s\")\n",
    "\n",
    "        # always append final boundary\n",
    "        new_boundaries.append(boundaries[-1])\n",
    "\n",
    "        stats = new_stats\n",
    "        boundaries = new_boundaries\n",
    "\n",
    "\n",
    "\n",
    "    return boundaries, stats, A_i_list, A_bkg_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "\n",
    "def compare_smoothing_windows(t, y, windows, decay_times, polyorder=2):\n",
    "    \"\"\"\n",
    "    Plot raw data and several smoothed versions to tune smoothing window.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array\n",
    "        Time axis\n",
    "    y : array\n",
    "        Raw intensity trace\n",
    "    windows : list of ints\n",
    "        Window lengths to test (must be odd)\n",
    "    polyorder : int\n",
    "        Polynomial order for Savitzky-Golay\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(t, np.log(y), color='black', alpha=0.4, label='Raw data')\n",
    "\n",
    "    for w in windows:\n",
    "        if w % 2 == 0:\n",
    "            w += 1  # enforce odd window\n",
    "        y_smooth = savgol_filter(y, window_length=w, polyorder=polyorder)\n",
    "        plt.plot(t, np.log(y_smooth), linewidth=2, label=f\"SG window={w}\")\n",
    "\n",
    "    for t in decay_times:\n",
    "        plt.vlines(t, ymin=np.min(np.log(y_smooth)), ymax=np.max(np.log(y_smooth)), color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "    plt.ylabel(\"Intensity [PSD]\")\n",
    "    plt.title(\"Comparison of smoothing windows\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the second run on, it is possible to rerun the code with the previous value of area found for each platau, to enhance precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Run\n",
    "USE_AREA_CONSISTENCY = False\n",
    "A_means_old = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Run \n",
    "USE_AREA_CONSISTENCY = True\n",
    "A_means_old = np.load(\"A_means.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ion number classifier: finds the number of ions for each plateau, and performs a check on lower plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_plateaus_hybrid(stats, boundaries, background_areas, A_means=None, k_sigma=7):\n",
    "    \"\"\"\n",
    "    Carlo's physically-correct classifier:\n",
    "\n",
    "    - If A_means exists: lowest plateau assigned to whichever cloud (0 or 1) it is closest to.\n",
    "    - If A_means is None: fallback to background-only classification.\n",
    "    - Higher plateaus assigned monotonically upward.\n",
    "    - Plateau 1 merged into plateau 0 only when A_means exists (needs A0_std).\n",
    "    - Monotonicity always preserved.\n",
    "    \"\"\"\n",
    "\n",
    "    means = np.array([s[\"mean_area\"] for s in stats])\n",
    "    N = len(means)\n",
    "    if N == 0:\n",
    "        return [], stats, boundaries\n",
    "\n",
    "    # -------------------------------\n",
    "    # Background stats\n",
    "    # -------------------------------\n",
    "    bg_mean = np.mean(background_areas)\n",
    "    bg_std  = np.std(background_areas, ddof=1) if len(background_areas) > 1 else 0.0\n",
    "    if np.isnan(bg_std):\n",
    "        bg_std = 0.0\n",
    "\n",
    "    # -------------------------------\n",
    "    # Step 1: classify lowest plateau\n",
    "    # -------------------------------\n",
    "    ion_counts = [None] * N\n",
    "    A_low = means[-1]\n",
    "\n",
    "    # A_means is usable only if it contains valid 0 and 1 distributions\n",
    "    have_Ameans = (\n",
    "        A_means is not None and\n",
    "        0 in A_means and len(A_means[0]) > 0 and\n",
    "        1 in A_means and len(A_means[1]) > 0\n",
    "    )\n",
    "\n",
    "    if have_Ameans:\n",
    "        # Use learned distributions\n",
    "        A0_mean = np.mean(A_means[0]); A0_std = np.std(A_means[0])\n",
    "        A1_mean = np.mean(A_means[1]); A1_std = np.std(A_means[1])\n",
    "\n",
    "        d0 = abs(A_low - A0_mean)\n",
    "        d1 = abs(A_low - A1_mean)\n",
    "\n",
    "        n_lowest = 0 if d0 <= d1 else 1\n",
    "\n",
    "    else:\n",
    "        # First run: background-only classification\n",
    "        if A_low < bg_mean + k_sigma * bg_std:\n",
    "            n_lowest = 0\n",
    "        else:\n",
    "            n_lowest = 1\n",
    "\n",
    "    print(\"A_low: \", A_low)\n",
    "    print(\"bg_mean + k_sigma * bg_std: \", bg_mean + k_sigma * bg_std)\n",
    "\n",
    "    ion_counts[-1] = n_lowest\n",
    "\n",
    "    # -------------------------------\n",
    "    # Step 2: assign higher plateaus monotonically upward\n",
    "    # -------------------------------\n",
    "    for i in range(N-2, -1, -1):\n",
    "        ion_counts[i] = ion_counts[i+1] + 1\n",
    "\n",
    "    # -------------------------------\n",
    "    # Step 3: merge plateau 1 into plateau 0 (only if A_means exists)\n",
    "    # -------------------------------\n",
    "    if N >= 2 and have_Ameans:\n",
    "        A0 = means[-1]      # lowest plateau\n",
    "        A1 = means[-2]      # next plateau\n",
    "        A0_std = np.std(A_means[0])\n",
    "\n",
    "        # plateau 1 is also close to 0-ion cloud\n",
    "        if abs(A1 - A0) < 1.1 * A0_std:\n",
    "            print(\"plateau merged\")\n",
    "            print(\"abs(A1 - A0): \", abs(A1 - A0))\n",
    "            print(\" 1.2 * A0_std: \",  1.2 * A0_std)\n",
    "            ion_counts[-2] = 0\n",
    "\n",
    "            # merge plateau 1 and plateau 0\n",
    "            stats[-2][\"t_end\"] = stats[-1][\"t_end\"]\n",
    "            boundaries[-2] = boundaries[-1]\n",
    "\n",
    "            # remove last plateau\n",
    "            stats = stats[:-1]\n",
    "            boundaries = boundaries[:-1]\n",
    "            ion_counts = ion_counts[:-1]\n",
    "\n",
    "            # shift all higher plateaus down by 1\n",
    "            for i in range(len(ion_counts)-1):\n",
    "                ion_counts[i] -= 1\n",
    "\n",
    "    return ion_counts, stats, boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 2D axes to 1D\n",
    "if f_axis.ndim == 2:\n",
    "    f_axis = f_axis[0, :]   # first row = frequency vector\n",
    "\n",
    "if t_axis.ndim == 2:\n",
    "    t_axis = t_axis[:, 0]   # first column = time vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_fine = 11    # window of the fine moving average\n",
    "window_coarse = 17  # window of the coarse moving average\n",
    "min_step_factor = 0.55   # factor for noise check\n",
    "tol = 0.041 # 40 ms     # tolerance for accepting values from fine and coarse\n",
    "min_drop = 0.2     # factor for accepting values\n",
    "tol_merging = 0.051     # tolerance for merging two values in coarse\n",
    "k_sigma = 7\n",
    "local_factor = 0.49\n",
    "max_factor = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "\n",
    "areas_list = []\n",
    "all_area_diffs = []   \n",
    "areas_bkg_list = []\n",
    "decay_times_list = []\n",
    "intensity_list, intensity_err_list = [], []\n",
    "all_plateau_diffs = []\n",
    "areas_list_final, all_area_diffs_final = [], [] \n",
    "transition_diffs = {}   # keys = (n, n-1), values = list of diffs\n",
    "A_means = {}\n",
    "A_vs_n_all = []\n",
    "def ion_label(n): return \"ion\" if n == 1 else \"ions\"\n",
    "\n",
    "for j, file in enumerate(filenames):\n",
    "    print(j)\n",
    "    y = summed_specs[j].T\n",
    "    spectrum = np.sum(summed_specs[j], axis=0)\n",
    "    idx_gs = np.argmax(spectrum)\n",
    "    df = f_axis[1] - f_axis[0]\n",
    "    offset_hz = 1800   # adjust if needed\n",
    "    offset_bins = int(offset_hz / abs(df))\n",
    "    idx_iso_center = idx_gs - offset_bins\n",
    "    half_width = 5\n",
    "    idx_1 = idx_iso_center - half_width\n",
    "    idx_2 = idx_iso_center + half_width\n",
    "    slice_i = slice(idx_1, idx_2)\n",
    "    y_i = y[slice_i]              # shape (M, N)\n",
    "    y_sum = y_i.sum(axis=0)\n",
    "    bkg_width = idx_2 - idx_1\n",
    "    slice_bkg = slice(idx_1 - bkg_width, idx_1)\n",
    "\n",
    "    # For plotting\n",
    "    slice_show = slice(idx_iso_center - 10, idx_iso_center + 15)\n",
    "\n",
    "    F_show = f_axis[slice_show]          # 1D frequency slice\n",
    "    T_show = t_axis                      # full time axis\n",
    "\n",
    "    XX, YY = np.meshgrid(F_show, T_show)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Smoothing (y_smooth) + local uncertainty (y_std)\n",
    "    # ---------------------------------------------------------\n",
    "    #kernel = np.ones(window) / window\n",
    "    #y_smooth = np.convolve(y_sum, kernel, mode='same')\n",
    "    y_smooth = savgol_filter(y_sum, window_length=window_fine, polyorder=2)\n",
    "\n",
    "    y_std = np.array([\n",
    "        y_sum[max(0, i - window_fine // 2):min(len(y_sum), i + window_fine // 2 + 1)].std(ddof=1)\n",
    "        / np.sqrt(window_fine)\n",
    "        for i in range(len(y_sum))\n",
    "    ])\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Decay detection\n",
    "    # ---------------------------------------------------------\n",
    "    bound_fine, stats, A_i_list, A_bkg_list = detect_all_ion_decays(\n",
    "        time=t_axis,\n",
    "        f_axis=f_axis,\n",
    "        y_sum=y_sum,\n",
    "        y=y,\n",
    "        slice_i=slice_i,\n",
    "        slice_bkg=slice_bkg,\n",
    "        min_points=7,\n",
    "        min_step_factor=min_step_factor,\n",
    "        use_smoothing=True,\n",
    "        smooth_window=window_fine,\n",
    "        require_decrease=True,\n",
    "        min_drop=min_drop,\n",
    "        local_factor=local_factor,\n",
    "        max_factor=max_factor\n",
    "        #min_rel_step=0.003 # 5% drop\n",
    "    )\n",
    "\n",
    "    means = [s[\"mean_area\"] for s in stats]\n",
    "    plateau_diffs = np.abs(np.diff(means))\n",
    "    all_plateau_diffs.extend(plateau_diffs)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # MULTI-SCALE DECAY VALIDATION\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # Coarse detection (more smoothing → fewer false positives)\n",
    "    bound_coarse, _, _, _ = detect_all_ion_decays(\n",
    "        time=t_axis,\n",
    "        f_axis=f_axis,\n",
    "        y_sum=y_sum,\n",
    "        y=y,\n",
    "        slice_i=slice_i,\n",
    "        slice_bkg=slice_bkg,\n",
    "        min_points=5,\n",
    "        min_step_factor=min_step_factor,\n",
    "        use_smoothing=True,\n",
    "        smooth_window=window_coarse,   # coarse smoothing\n",
    "        require_decrease=True,\n",
    "        min_drop=min_drop,\n",
    "        local_factor=local_factor,\n",
    "        max_factor=max_factor\n",
    "    )\n",
    "\n",
    "    def ensure_indices(bounds):\n",
    "        out = []\n",
    "        for b in bounds:\n",
    "            if isinstance(b, (int, np.integer)):\n",
    "                out.append(b)\n",
    "            else:\n",
    "                # b is a time → convert to nearest index\n",
    "                out.append(int(np.argmin(np.abs(t_axis - b))))\n",
    "        return out\n",
    "\n",
    "    bound_fine = ensure_indices(bound_fine)\n",
    "    bound_coarse = ensure_indices(bound_coarse)\n",
    "\n",
    "\n",
    "    # Convert boundaries → decay times\n",
    "    def boundaries_to_times(bounds):\n",
    "        return [t_axis[b] for b in bounds[1:-1]]\n",
    "\n",
    "    decays_fine   = boundaries_to_times(bound_fine)\n",
    "    print(\"decays_fine: \", decays_fine)\n",
    "    decays_coarse = boundaries_to_times(bound_coarse)\n",
    "    print(\"decays_coarse: \", decays_coarse)\n",
    "    merged = []\n",
    "    for t in decays_coarse:\n",
    "        if not merged:\n",
    "            merged.append(t)\n",
    "        else:\n",
    "            prev = merged[-1]\n",
    "\n",
    "            if abs(t - prev) < tol_merging:\n",
    "\n",
    "                # Check if either coarse value matches a fine decay\n",
    "                matches_prev = [tf for tf in decays_fine if abs(tf - prev) < tol]\n",
    "                matches_t    = [tf for tf in decays_fine if abs(tf - t)    < tol]\n",
    "\n",
    "                if matches_prev:\n",
    "                    # keep midpoint of prev and its fine match\n",
    "                    merged[-1] = 0.5 * (prev + matches_prev[0])\n",
    "\n",
    "                elif matches_t:\n",
    "                    # keep midpoint of t and its fine match\n",
    "                    merged[-1] = 0.5 * (t + matches_t[0])\n",
    "\n",
    "                else:\n",
    "                    # normal merge\n",
    "                    merged[-1] = 0.5 * (t + prev)\n",
    "\n",
    "            else:\n",
    "                merged.append(t)\n",
    "\n",
    "    decays_coarse = merged\n",
    "    print(\"decays_coarse after merging:\", decays_coarse)\n",
    "\n",
    "\n",
    "    # Keep only decays that appear in both (within tolerance)\n",
    "    final_decays = []\n",
    "\n",
    "    used_fine = set()   # <-- NEW: track which fine decays have been used\n",
    "\n",
    "    for tc in decays_coarse:\n",
    "\n",
    "        # find all fine decays close to this coarse decay\n",
    "        close = [(tf, abs(tf - tc)) for tf in decays_fine\n",
    "                if tf not in used_fine and abs(tf - tc) < tol]\n",
    "\n",
    "        if len(close) == 1:\n",
    "            tf, _ = close[0]\n",
    "            final_decays.append((tf + tc) / 2)\n",
    "            used_fine.add(tf)   # <-- mark as used\n",
    "\n",
    "        elif len(close) > 1:\n",
    "            # keep only the closest fine decay\n",
    "            tf, _ = min(close, key=lambda x: x[1])\n",
    "            final_decays.append((tf + tc) / 2)\n",
    "            used_fine.add(tf)   # <-- mark as used\n",
    "\n",
    "    # if len(close) == 0 → skip (coarse-only decay)\n",
    "\n",
    "    print(\"Decay times: \", final_decays)\n",
    "    \n",
    "    # Rebuild boundaries from final_decays\n",
    "    boundaries = [0] + [np.argmin(abs(t_axis - t)) for t in final_decays] + [len(t_axis)-1]\n",
    "    boundaries = sorted(set(boundaries))\n",
    "    #print(\"Boundaries 1: \", boundaries)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Rebuild stats to match the NEW boundaries\n",
    "    # ---------------------------------------------------------\n",
    "    new_stats = []\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        s = boundaries[i]\n",
    "        e = boundaries[i + 1]\n",
    "\n",
    "        new_stats.append({\n",
    "            \"t_start\": t_axis[s],\n",
    "            \"t_end\":   t_axis[e - 1],\n",
    "            \"mean_y\":  y_sum[s:e].mean(),\n",
    "            \"std_y\":   y_sum[s:e].std(ddof=1),\n",
    "            \"mean_area\":      A_i_list[s:e].mean(),\n",
    "            \"mean_area_bkg\":  A_bkg_list[s:e].mean()\n",
    "        })\n",
    "\n",
    "    stats = new_stats\n",
    "\n",
    "    #decay_times = [t_axis[b] for b in boundaries[1:-1]]\n",
    "    #decay_times_list.append(decay_times)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Area differences\n",
    "    # ---------------------------------------------------------\n",
    "    areas_bkg = [seg[\"mean_area_bkg\"] for seg in stats]\n",
    "    areas_bkg_list.append(areas_bkg)\n",
    "\n",
    "    intensity = [seg[\"mean_y\"] for seg in stats]\n",
    "    intensity_err = [seg[\"std_y\"] for seg in stats]\n",
    "    intensity_list.append(intensity)\n",
    "    intensity_err_list.append(intensity_err)\n",
    "\n",
    "    ion_counts, stats, boundaries = classify_plateaus_hybrid(stats, boundaries=boundaries, background_areas=areas_bkg, A_means=A_means_old)\n",
    "\n",
    "    areas = [seg[\"mean_area\"] for seg in stats]\n",
    "    areas_list.append(areas)\n",
    "    area_diffs = [areas[i+1] - areas[i] for i in range(len(areas)-1)]\n",
    "    all_area_diffs.append(area_diffs)\n",
    "\n",
    "    # --- Save area → ion mapping for this file ---\n",
    "    A_vs_n = []\n",
    "\n",
    "    for A, n in zip(areas, ion_counts):\n",
    "        A_vs_n.append((n, A))\n",
    "        A_means.setdefault(n, []).append(A)\n",
    "\n",
    "    # Store it in a global list if you want to analyze later\n",
    "    A_vs_n_all.append(A_vs_n)\n",
    "\n",
    "    # --------------------------------------------------------- \n",
    "    # # Merge consecutive plateaus with same ion count (time order) \n",
    "    # # --------------------------------------------------------- \n",
    "    #boundaries, stats, ion_counts = merge_consecutive_plateaus(boundaries, stats, ion_counts)\n",
    "\n",
    "    # FIX: remove duplicate boundaries\n",
    "    cleaned = []\n",
    "    for b in boundaries:\n",
    "        if not cleaned or cleaned[-1] != b:\n",
    "            cleaned.append(b)\n",
    "    boundaries = cleaned\n",
    "    # ---------------------------------------------------------\n",
    "    # Collect area differences grouped by ion transition\n",
    "    # ---------------------------------------------------------\n",
    "    means = [s[\"mean_area\"] for s in stats]\n",
    "\n",
    "    for i in range(len(stats) - 1):\n",
    "        n1 = ion_counts[i]\n",
    "        n2 = ion_counts[i+1]\n",
    "\n",
    "        # Only consider real decays: ion count must decrease by 1\n",
    "        if n1 - n2 == 1:\n",
    "            diff = abs(means[i] - means[i+1])\n",
    "            key = (n1, n2)\n",
    "\n",
    "            if key not in transition_diffs:\n",
    "                transition_diffs[key] = []\n",
    "\n",
    "            transition_diffs[key].append(diff)\n",
    "\n",
    "    areas_final = [seg[\"mean_area\"] for seg in stats]\n",
    "    areas_list_final.append(areas_final)\n",
    "    area_diffs_final = [areas[i+1] - areas[i] for i in range(len(areas)-1)]\n",
    "    all_area_diffs_final.append(area_diffs_final)\n",
    "    \n",
    "    # # recompute decay_times after merge \n",
    "    #decay_times = [t_axis[b] for b in boundaries[1:-1]] \n",
    "    decay_times = [t_axis[b] for b in boundaries[1:-1]]\n",
    "    decay_times_list.append(decay_times)\n",
    "    '''decay_times = []\n",
    "    for i in range(1, len(stats)):\n",
    "        if ion_counts[i] != ion_counts[i-1]:\n",
    "            # transition between plateau i-1 and i\n",
    "            t_left  = stats[i-1][\"t_end\"]\n",
    "            t_right = stats[i][\"t_start\"]\n",
    "            t_mid = 0.5 * (t_left + t_right)\n",
    "            decay_times.append(t_mid)\n",
    "    '''\n",
    "    #print(\"Boundaries:\", boundaries)\n",
    "    #print(\"Decay times after merging:\", decay_times)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Create side-by-side figure\n",
    "    # ---------------------------------------------------------\n",
    "    fig, (ax_left, ax_right) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # =========================================================\n",
    "    # LEFT PANEL — Intensity over time\n",
    "    # =========================================================\n",
    "    ax_left.plot(t_axis, (y_smooth), linewidth=2, color='green', label='Moving average')\n",
    "    ax_left.fill_between(t_axis, (y_smooth - y_std), (y_smooth + y_std),\n",
    "                         color='green', alpha=0.2, label='Uncertainty')\n",
    "    ax_left.plot(t_axis, (y_sum), label='Data', color='blue')\n",
    "    #print(\"decayyyyy\", decay_times)\n",
    "    for t in decay_times:\n",
    "        ax_left.axvline(t, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    legend_entries = []\n",
    "    header = Line2D([], [], color='none')\n",
    "    legend_entries.append((header, \"Interval            |            Mean PSD             |  Mean Area\"))\n",
    "\n",
    "    for seg, ions in zip(stats, ion_counts):\n",
    "        row = Line2D([], [], color='none')\n",
    "        label = (f\"{seg['t_start']:.2f}–{seg['t_end']:.2f} s  |  \"\n",
    "                f\"{seg['mean_y']:.2e} ± {seg['std_y']:.2e}  |  \"\n",
    "                f\"{seg['mean_area']:.2e}  |  {ions} ions\")\n",
    "        legend_entries.append((row, label))\n",
    "\n",
    "    # annotate ions in intensity plot\n",
    "    for i, seg in enumerate(stats):\n",
    "        t0 = seg[\"t_start\"]\n",
    "        t1 = seg[\"t_end\"]\n",
    "        t_mid = 0.5 * (t0 + t1)\n",
    "\n",
    "        ions = ion_counts[i]\n",
    "\n",
    "        ax_left.text(\n",
    "            t_mid,\n",
    "            ax_left.get_ylim()[0],          # slightly above the curve\n",
    "            f\"{ions} {ion_label(ions)}\",\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=10,\n",
    "            bbox=dict(facecolor='white', alpha=0.95, edgecolor='black')\n",
    "        )\n",
    "\n",
    "    # annotate ions in spectrogram plot\n",
    "    for i, seg in enumerate(stats):\n",
    "        t0 = seg[\"t_start\"]\n",
    "        t1 = seg[\"t_end\"]\n",
    "        t_mid = 0.5 * (t0 + t1)\n",
    "\n",
    "        ions = ion_counts[i]\n",
    "\n",
    "        ax_right.text(\n",
    "            f_axis[slice_show].min(),   # left side of spectrogram\n",
    "            t_mid,\n",
    "            f\"{ions} {ion_label(ions)}\",\n",
    "            ha='left',\n",
    "            va='center',\n",
    "            fontsize=10,\n",
    "            color='white',\n",
    "            bbox=dict(facecolor='black', alpha=0.95, edgecolor='white')\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    handles = [h for h, _ in legend_entries]\n",
    "    labels = [l for _, l in legend_entries]\n",
    "    ax_left.legend(handles, labels, loc='upper right', fontsize=8, frameon=True)\n",
    "\n",
    "    ax_left.grid()\n",
    "    ax_left.set_xlabel(\"Time [s]\")\n",
    "    ax_left.set_ylabel(\"Intensity [PSD]\")\n",
    "    ax_left.set_title(\"Isomer intensity over time with decay detection\")\n",
    "    # Convert area_diffs to a readable string\n",
    "    area_diff_str = \", \".join([f\"{-d:.2e}\" for d in area_diffs])\n",
    "    # Add to legend\n",
    "    row = Line2D([], [], color='none')\n",
    "    label = f\"Area diffs: {area_diff_str}\"\n",
    "    legend_entries.append((row, label))\n",
    "    handles = [h for h, _ in legend_entries]\n",
    "    labels = [l for _, l in legend_entries]\n",
    "    ax_left.legend(handles, labels, loc='upper right', fontsize=8, frameon=True)\n",
    "\n",
    "    # =========================================================\n",
    "    # RIGHT PANEL — Spectrogram\n",
    "    # =========================================================\n",
    "    plotters.plot_spectrogram(\n",
    "        XX.T, YY.T, summed_specs[j][:,slice_show].T,\n",
    "        cen=0.0,\n",
    "        cmap=\"jet\",\n",
    "        dpi=300,\n",
    "        dbm=False,\n",
    "        filename=None,\n",
    "        zzmin=1000,\n",
    "        zzmax=20000,\n",
    "        mask=False,\n",
    "        decimal_place=2\n",
    "    )\n",
    "\n",
    "    ax_right.set_title(\"Spectrogram\")\n",
    "    ax_right.set_xticklabels(ax_right.get_xticklabels(), rotation=45)\n",
    "    ax_right.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.1f} Hz\"))\n",
    "\n",
    "    ax_right.vlines(x=f_axis[idx_1], ymin=0, ymax=t_axis[-1], color='red')\n",
    "    ax_right.vlines(x=f_axis[idx_2], ymin=0, ymax=t_axis[-1], color='red')\n",
    "    ax_right.vlines(x=f_axis[idx_gs], ymin=0, ymax=t_axis[-1], color='yellow')\n",
    "\n",
    "    for t in decay_times: \n",
    "        ax_right.axhline(t, color='white', linestyle='--', linewidth=1)\n",
    "\n",
    "    fig.suptitle(file.split(\"/\")[-1], fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(y_sum, bins=60, color='steelblue', alpha=0.75, label='Data')\n",
    "    plt.hist(y_smooth, bins=60, color='green', alpha=0.75, label='Moving avg')\n",
    "\n",
    "    plt.xlabel(\"Intensity [PSD]\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(\"Histogram of intensity values with ion labels\")\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Add plateau markers\n",
    "    for seg, ions in zip(stats, ion_counts):\n",
    "        mean_y = seg[\"mean_y\"]\n",
    "        plt.axvline(mean_y, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "        plt.text(\n",
    "            mean_y,\n",
    "            plt.ylim()[1] * 0.9,     # near the top of the histogram\n",
    "            f\"{ions} {ion_label(ions)}\",\n",
    "            ha='center',\n",
    "            \n",
    "            va='bottom',\n",
    "            fontsize=10,\n",
    "            bbox=dict(facecolor='white', alpha=0.9, edgecolor='black')\n",
    "        )\n",
    "    #compare_smoothing_windows(t_axis, y_sum, windows=[9, 17], decay_times=decay_times)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# After the loop\n",
    "# ---------------------------------------------------------\n",
    "print(\"Area differences for each file:\")\n",
    "for fname, diffs in zip(filenames, all_area_diffs):\n",
    "    print(fname.split(\"/\")[-1], diffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ion, areas in A_means.items():\n",
    "    #print(\"ion number: \", ion)\n",
    "    #print(\"areas: \", areas)\n",
    "    mean_A = np.mean(areas)\n",
    "    std_A  = np.std(areas, ddof=1)\n",
    "    print(f\"Ion {ion}: mean = {mean_A:.6e}, std = {std_A:.6e}, count = {len(areas)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"A_means.npy\", A_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# LEFT PANEL — Mean plateau area vs ion number\n",
    "# ============================================================\n",
    "\n",
    "ion_numbers = sorted(A_means.keys())\n",
    "mean_areas = np.array([np.mean(A_means[n]) for n in ion_numbers])\n",
    "std_areas  = np.array([np.std(A_means[n], ddof=1) for n in ion_numbers])\n",
    "\n",
    "# Linear fit\n",
    "a_lin, b_lin = np.polyfit(ion_numbers, mean_areas, deg=1)\n",
    "\n",
    "# Quadratic fit\n",
    "a2, b2, c2 = np.polyfit(ion_numbers, mean_areas, deg=2)\n",
    "\n",
    "n_fit = np.linspace(min(ion_numbers), max(ion_numbers), 200)\n",
    "A_fit_lin  = a_lin*n_fit + b_lin\n",
    "A_fit_quad = a2*n_fit**2 + b2*n_fit + c2\n",
    "\n",
    "print(f\"Linear fit:     A(n) = {a_lin:.3e} n + {b_lin:.3e}\")\n",
    "print(f\"Quadratic fit:  A(n) = {a2:.3e} n^2 + {b2:.3e} n + {c2:.3e}\")\n",
    "\n",
    "# ============================================================\n",
    "# RIGHT PANEL — ΔA(n) computed exactly as you did it\n",
    "# ============================================================\n",
    "\n",
    "# Your method: simply take the diff of mean_areas\n",
    "area_diffs = np.diff(mean_areas)\n",
    "\n",
    "# x-values for the diffs: transitions 1→0, 2→1, 3→2, ...\n",
    "# If ion_numbers = [0,1,2,3,4,5], then diffs correspond to n = [1,2,3,4,5]\n",
    "n_vals = ion_numbers[1:]   # skip the first one\n",
    "\n",
    "# Build labels like \"1→0\", \"2→1\", ...\n",
    "labels = [f\"{n}→{n-1}\" for n in n_vals]\n",
    "\n",
    "# ============================================================\n",
    "# COMBINED FIGURE\n",
    "# ============================================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "# ---------------- LEFT PANEL ----------------\n",
    "ax1.errorbar(\n",
    "    ion_numbers, mean_areas, yerr=std_areas,\n",
    "    fmt='o', markersize=8, capsize=5, label=\"Mean area ± 1 std\"\n",
    ")\n",
    "ax1.plot(n_fit, A_fit_lin,  color='green', linestyle='--', linewidth=2, label=\"Linear fit\")\n",
    "ax1.plot(n_fit, A_fit_quad, color='red',   linestyle='--', linewidth=2, label=\"Quadratic fit\")\n",
    "\n",
    "ax1.set_xlabel(\"Number of ions\")\n",
    "ax1.set_ylabel(\"Mean area\")\n",
    "ax1.set_title(\"Mean plateau area vs ion number\")\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# ---------------- RIGHT PANEL ----------------\n",
    "ax2.plot(\n",
    "    n_vals, area_diffs,\n",
    "    'o-', markersize=8, label=\"ΔA(n) = A(n) - A(n-1)\"\n",
    ")\n",
    "\n",
    "ax2.set_xticks(n_vals)\n",
    "ax2.set_xticklabels(labels)\n",
    "\n",
    "ax2.set_ylabel(\"Area difference\")\n",
    "ax2.set_xlabel(\"Ion transition\")\n",
    "ax2.set_title(\"Mean area difference per ion transition\")\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "colors = [\"blue\", \"green\", \"orange\", \"pink\"]\n",
    "\n",
    "# --- compute stats ---\n",
    "stats = {}\n",
    "for key in [(1,0), (2,1), (3,2), (4,3)]:\n",
    "    if key in transition_diffs and len(transition_diffs[key]) > 0:\n",
    "        arr = np.array(transition_diffs[key])\n",
    "        arr = arr[arr < np.percentile(arr, 80)]\n",
    "\n",
    "        stats[key] = {\n",
    "            \"mean\": np.mean(arr),\n",
    "            \"var\": np.var(arr, ddof=1),\n",
    "            \"std\": np.std(arr, ddof=1),\n",
    "            \"n\": len(arr)\n",
    "        }\n",
    "\n",
    "# --- plot histograms ---\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for idx, key in enumerate([(1,0), (2,1), (3,2), (4,3)]):\n",
    "    if key in transition_diffs:\n",
    "        mean = stats[key][\"mean\"]\n",
    "        std  = stats[key][\"std\"]\n",
    "        label = f\"{key[0]}→{key[1]}  (μ={mean:.2e}, σ={std:.2e})\"\n",
    "\n",
    "        plt.hist(\n",
    "            transition_diffs[key],\n",
    "            bins=40,\n",
    "            alpha=0.4,\n",
    "            color=colors[idx],\n",
    "            label=label\n",
    "        )\n",
    "\n",
    "        # vertical line at the mean\n",
    "        plt.axvline(\n",
    "            mean,\n",
    "            color=colors[idx],\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.xlabel(\"Area difference\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Area differences grouped by ion transition\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(means)\n",
    "ratio = []\n",
    "for i in range(len(means)-1):\n",
    "    ratio.append(means[i+1]/means[i])\n",
    "\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_means = {}\n",
    "for ion_count, area in zip(ion_counts, areas):\n",
    "    A_means.setdefault(ion_count, []).append(area)\n",
    "\n",
    "means = []\n",
    "for n, arr in A_means.items():\n",
    "    print(n, np.mean(arr))\n",
    "    means.append(np.mean(arr))\n",
    "\n",
    "ratio = []\n",
    "for i in range(len(means)-1):\n",
    "    ratio.append(means[i+1]/means[i])\n",
    "\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "flat = []\n",
    "for x in (all_plateau_diffs):\n",
    "    flat.append(x)\n",
    "#flat = [x for diffs in np.array(all_plateau_diffs) for x in diffs]\n",
    "flat = np.array(flat)\n",
    "flat_sorted = sorted(flat)\n",
    "plt.hist(np.array(flat_sorted)[:-2], bins=60, density=True, alpha=0.4, color='steelblue', label='Histogram')\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "kde = gaussian_kde(np.array(flat_sorted)[:-2])\n",
    "xs = np.linspace(min(np.array(flat_sorted)[:-2]), max(np.array(flat_sorted)[:-2]), 500)\n",
    "#plt.plot(xs, kde(xs), color='red', linewidth=2, label='KDE')\n",
    "\n",
    "plt.xlabel(\"Area difference\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of area differences across all files\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = np.array(flat_sorted)[:-2].reshape(-1,1)\n",
    "kmeans = KMeans(n_clusters=4).fit(X)\n",
    "centers = sorted(kmeans.cluster_centers_.flatten())\n",
    "\n",
    "print(\"Cluster centers:\", centers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "%matplotlib inline\n",
    "plateau_means = []\n",
    "step_starts = [] \n",
    "step_ends = []\n",
    "\n",
    "y = np.array(sorted(flat)[::-1])\n",
    "x = np.arange(len(y))\n",
    "\n",
    "y_smooth = savgol_filter(y, window_length=11, polyorder=2)\n",
    "dy = np.diff(y)\n",
    "\n",
    "noise = np.median(np.abs(dy - np.median(dy))) * 1.4826\n",
    "neg_threshold = -1.5 * noise\n",
    "flat_threshold = 1 * noise\n",
    "\n",
    "i = 0\n",
    "while i < len(dy):\n",
    "    # detect start of a downward step\n",
    "    if dy[i] < neg_threshold:\n",
    "        step_starts.append(i)\n",
    "\n",
    "        # find where derivative returns to near zero\n",
    "        j = i + 1\n",
    "        while j < len(dy) and abs(dy[j]) > flat_threshold:\n",
    "            j += 1\n",
    "\n",
    "        step_ends.append(j)\n",
    "        i = j\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "boundaries = [0]\n",
    "for s, e in zip(step_starts, step_ends):\n",
    "    boundaries.append(s)   # plateau ends before step\n",
    "    boundaries.append(e)   # next plateau starts after step\n",
    "boundaries.append(len(y))\n",
    "\n",
    "plateau_means = []\n",
    "for i in range(0, len(boundaries)-1, 2):\n",
    "    s = boundaries[i]\n",
    "    e = boundaries[i+1]\n",
    "    plateau_means.append(y[s:e].mean())\n",
    "\n",
    "step_sizes = np.diff(plateau_means)[1:]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, y, label=\"Data\")\n",
    "#plt.plot(x, y_smooth, label=\"Smoothed\", alpha=0.7)\n",
    "\n",
    "# plot plateaus\n",
    "for i in range(0, len(boundaries)-1, 2):\n",
    "    s = boundaries[i]\n",
    "    e = boundaries[i+1]\n",
    "    plt.hlines(plateau_means[i//2], s, e, colors='red', linewidth=2, linestyle=\"--\")\n",
    "\n",
    "# plot step transitions\n",
    "for s, e in zip(step_starts, step_ends):\n",
    "    plt.axvline(s, color='green', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(e, color='green', linestyle='--', alpha=0.3)\n",
    "\n",
    "#plt.gca().invert_xaxis()\n",
    "plt.xlabel(\"Number of Particles\")\n",
    "plt.ylabel(\"Area Values\")\n",
    "plt.title(\"Plateaus and Step Transitions\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = [t for sub in decay_times_list for t in sub]\n",
    "t = np.asarray(flat)\n",
    "lam = 1 / t.mean()\n",
    "half_life = np.log(2) * t.mean()\n",
    "half_life_err = half_life / np.sqrt(len(t))\n",
    "x = np.linspace(0, max(flat), 500)\n",
    "plt.plot(x, lam * np.exp(-lam * x))\n",
    "\n",
    "bins = int(np.sqrt(len(flat)))  # or Freedman–Diaconis\n",
    "plt.hist(flat, bins=bins, density=True, alpha=0.5, label=f\"Half-life= {half_life*1000:.2f} ± {half_life_err*1000:.2f} ms\")\n",
    "plt.xlabel(\"Decay time [s]\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Histogram of decay times\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
